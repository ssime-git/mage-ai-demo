{"block_file": {"data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "/home/src/mlops_demo/data_loaders/make_dataset.py:data_loader:python:home/src/mlops demo/data loaders/make dataset": {"content": "import io\nimport pandas as pd\nimport requests\nimport numpy as np\nfrom sklearn.datasets import make_classification\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_customer_data(*args, **kwargs):\n    \"\"\"\n    Generate synthetic customer data for churn prediction\n    \"\"\"\n    # Generate synthetic dataset\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=10,\n        n_informative=8,\n        n_redundant=2,\n        n_clusters_per_class=1,\n        random_state=42\n    )\n    \n    # Create DataFrame with meaningful column names\n    feature_names = [\n        'account_age', 'monthly_charges', 'total_charges', 'num_services',\n        'customer_service_calls', 'contract_length', 'payment_method_score',\n        'usage_frequency', 'support_tickets', 'satisfaction_score'\n    ]\n    \n    df = pd.DataFrame(X, columns=feature_names)\n    df['customer_id'] = range(1, len(df) + 1)\n    df['churn'] = y\n    \n    # Add some realistic data transformations\n    df['account_age'] = np.abs(df['account_age'] * 12).astype(int)  # months\n    df['monthly_charges'] = np.abs(df['monthly_charges'] * 50 + 100)  # dollars\n    df['total_charges'] = df['monthly_charges'] * df['account_age']\n    df['num_services'] = np.abs(df['num_services']).astype(int) % 10 + 1\n    \n    print(f\"Loaded {len(df)} customer records\")\n    print(f\"Churn rate: {df['churn'].mean():.2%}\")\n    \n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'Data loading failed'\n    assert len(output) > 0, 'No data loaded'\n    assert 'churn' in output.columns, 'Target variable missing'\n    print(f\"\u2705 Data validation passed: {len(output)} records loaded\")\n", "file_path": "/home/src/mlops_demo/data_loaders/make_dataset.py", "language": "python", "type": "data_loader", "uuid": "make_dataset"}, "/home/src/mlops_demo/transformers/data_preproecessing.py:transformer:python:home/src/mlops demo/transformers/data preproecessing": {"content": "import joblib\nimport os\n\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef preprocess_data(df: DataFrame, *args, **kwargs) -> dict:\n    \"\"\"\n    Preprocess customer data for ML training\n    \"\"\"\n    # Create preprocessing directory\n    os.makedirs('/home/src/mlops_demo/models', exist_ok=True)\n    \n    # Separate features and target\n    feature_cols = [col for col in df.columns if col not in ['customer_id', 'churn']]\n    X = df[feature_cols]\n    y = df['churn']\n    \n    # Handle missing values\n    X = X.fillna(X.median())\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Save the scaler for later use\n    joblib.dump(scaler, '/home/src/mlops_demo/models/scaler.pkl')\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    print(f\"Training set: {X_train.shape[0]} samples\")\n    print(f\"Test set: {X_test.shape[0]} samples\")\n    print(f\"Features: {len(feature_cols)}\")\n    \n    # Convert numpy arrays to lists for JSON serialization\n    # But keep the original arrays for ML training\n    return {\n        'X_train': X_train.tolist(),\n        'X_test': X_test.tolist(),\n        'y_train': y_train.tolist(),\n        'y_test': y_test.tolist(),\n        'feature_names': feature_cols,\n        'scaler_path': '/home/src/mlops_demo/models/scaler.pkl',\n        'data_shapes': {\n            'X_train_shape': X_train.shape,\n            'X_test_shape': X_test.shape,\n            'y_train_shape': y_train.shape,\n            'y_test_shape': y_test.shape\n        }\n    }\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'Output is None'\n    assert isinstance(output, dict), 'Output should be a dictionary'\n    assert 'X_train' in output, 'Training features missing'\n    assert 'y_train' in output, 'Training target missing'\n    assert 'X_test' in output, 'Test features missing'\n    assert 'y_test' in output, 'Test target missing'\n    assert 'data_shapes' in output, 'Data shapes missing'\n    \n    # Check data shapes using the stored shape info\n    shapes = output['data_shapes']\n    assert shapes['X_train_shape'][0] > 0, 'No training data'\n    assert shapes['X_test_shape'][0] > 0, 'No test data'\n    assert shapes['y_train_shape'][0] > 0, 'No training labels'\n    assert shapes['y_test_shape'][0] > 0, 'No test labels'\n    \n    # Check that data is in list format (JSON serializable)\n    assert isinstance(output['X_train'], list), 'X_train should be a list'\n    assert isinstance(output['y_train'], list), 'y_train should be a list'\n    \n    print(f\"\u2705 Preprocessing validation passed\")\n    print(f\"   Training samples: {shapes['X_train_shape'][0]}\")\n    print(f\"   Test samples: {shapes['X_test_shape'][0]}\")\n    print(f\"   Features: {len(output['feature_names'])}\")\n\n", "file_path": "/home/src/mlops_demo/transformers/data_preproecessing.py", "language": "python", "type": "transformer", "uuid": "data_preproecessing"}, "/home/src/mlops_demo/transformers/model_training.py:transformer:python:home/src/mlops demo/transformers/model training": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\nimport joblib\nimport json\nimport os\n\n@transformer\ndef train_model(data: dict, *args, **kwargs) -> dict:\n    \"\"\"\n    Train machine learning model for churn prediction\n    \"\"\"\n    # Convert lists back to numpy arrays for ML training\n    import numpy as np\n    X_train = np.array(data['X_train'])\n    y_train = np.array(data['y_train'])\n    X_test = np.array(data['X_test'])\n    y_test = np.array(data['y_test'])\n    \n    # Initialize and train model\n    model = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    print(\"Training model...\")\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_pred_proba)\n    \n    # Feature importance\n    feature_importance = dict(zip(\n        data['feature_names'], \n        model.feature_importances_\n    ))\n    \n    # Sort by importance\n    feature_importance = dict(sorted(\n        feature_importance.items(), \n        key=lambda x: x[1], \n        reverse=True\n    ))\n    \n    # Save model\n    model_path = '/home/src/mlops_demo/models/churn_model.pkl'\n    joblib.dump(model, model_path)\n    \n    # Save metrics\n    metrics = {\n        'accuracy': float(accuracy),\n        'auc_score': float(auc_score),\n        'feature_importance': feature_importance,\n        'model_path': model_path,\n        'training_samples': len(X_train),\n        'test_samples': len(X_test)\n    }\n    \n    with open('/home/src/mlops_demo/models/metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    print(f\"Model trained successfully!\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"AUC Score: {auc_score:.4f}\")\n    print(f\"Top 3 features: {list(feature_importance.keys())[:3]}\")\n    \n    return metrics\n\n@test\ndef test_output(output, *args) -> None:\n    assert 'accuracy' in output, 'Accuracy metric missing'\n    assert output['accuracy'] > 0.5, 'Model accuracy too low'\n    assert 'model_path' in output, 'Model path missing'\n    print(f\"\u2705 Model training validation passed\")\n", "file_path": "/home/src/mlops_demo/transformers/model_training.py", "language": "python", "type": "transformer", "uuid": "model_training"}, "/home/src/mlops_demo/data_loaders/load_model_and_make_prediction.py:data_loader:python:home/src/mlops demo/data loaders/load model and make prediction": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nimport joblib\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\n\n@data_loader\ndef predict_churn(*args, **kwargs):\n    \"\"\"\n    Load model and make predictions via API trigger\n    \"\"\"\n    # Get prediction data from API call variables\n    if 'input_data' in kwargs:\n        input_data = kwargs['input_data']\n    else:\n        # Default test data for demonstration\n        input_data = {\n            \"account_age\": 24,\n            \"monthly_charges\": 85.5,\n            \"total_charges\": 2052.0,\n            \"num_services\": 5,\n            \"customer_service_calls\": 2,\n            \"contract_length\": 12,\n            \"payment_method_score\": 0.8,\n            \"usage_frequency\": 0.7,\n            \"support_tickets\": 1,\n            \"satisfaction_score\": 0.6\n        }\n    \n    # Load latest model from registry\n    registry_path = '/home/src/mlops_demo/model_registry'\n    latest_path = os.path.join(registry_path, 'latest.json')\n    \n    if not os.path.exists(latest_path):\n        raise Exception(\"No trained model found. Please run the training pipeline first.\")\n    \n    with open(latest_path, 'r') as f:\n        latest_info = json.load(f)\n    \n    # Load model and scaler\n    model_path = os.path.join(latest_info['path'], 'model.pkl')\n    scaler_path = os.path.join(latest_info['path'], 'scaler.pkl')\n    \n    model = joblib.load(model_path)\n    scaler = joblib.load(scaler_path)\n    \n    # Feature names (should match training)\n    feature_names = [\n        'account_age', 'monthly_charges', 'total_charges', 'num_services',\n        'customer_service_calls', 'contract_length', 'payment_method_score',\n        'usage_frequency', 'support_tickets', 'satisfaction_score'\n    ]\n    \n    # Prepare input data\n    df = pd.DataFrame([input_data])\n    \n    # Ensure all features are present\n    for feature in feature_names:\n        if feature not in df.columns:\n            df[feature] = 0\n    \n    # Select and order features\n    X = df[feature_names].values\n    \n    # Scale features\n    X_scaled = scaler.transform(X)\n    \n    # Make prediction\n    prediction = model.predict(X_scaled)[0]\n    probability = model.predict_proba(X_scaled)[0]\n    \n    result = {\n        'customer_data': input_data,\n        'prediction': int(prediction),\n        'probability': {\n            'no_churn': float(probability[0]),\n            'churn': float(probability[1])\n        },\n        'risk_level': 'High' if probability[1] > 0.7 else 'Medium' if probability[1] > 0.3 else 'Low',\n        'model_version': latest_info['version'],\n        'prediction_timestamp': pd.Timestamp.now().isoformat()\n    }\n    \n    print(f\"Prediction made: {result}\")\n    return result\n\n@test\ndef test_output(output, *args) -> None:\n    assert 'prediction' in output, 'Prediction missing'\n    assert 'probability' in output, 'Probability missing'\n    assert 'risk_level' in output, 'Risk level missing'\n    print(\"\u2705 Prediction service validation passed\")\n", "file_path": "/home/src/mlops_demo/data_loaders/load_model_and_make_prediction.py", "language": "python", "type": "data_loader", "uuid": "load_model_and_make_prediction"}, "/home/src/mlops_demo/markdowns/how_to_use.md:markdown:markdown:home/src/mlops demo/markdowns/how to use": {"content": "# How to use\n\n```bash\n# Test prediction API\ncurl -X POST http://localhost:6789/api/pipeline_schedules/1/pipeline_runs/c92cc53194484103a63d3cf4446a3b4f \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"pipeline_run\": {\n      \"variables\": {\n        \"input_data\": {\n          \"account_age\": 36,\n          \"monthly_charges\": 120.0,\n          \"total_charges\": 4320.0,\n          \"num_services\": 8,\n          \"customer_service_calls\": 5,\n          \"contract_length\": 6,\n          \"payment_method_score\": 0.3,\n          \"usage_frequency\": 0.4,\n          \"support_tickets\": 3,\n          \"satisfaction_score\": 0.2\n        }\n      }\n    }\n  }'\n```", "file_path": "/home/src/mlops_demo/markdowns/how_to_use.md", "language": "markdown", "type": "markdown", "uuid": "how_to_use"}, "/home/src/mlops_demo/data_exporters/model_registry.py:data_exporter:python:home/src/mlops demo/data exporters/model registry": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\nimport json\nimport os\nfrom datetime import datetime\nimport joblib\n\n@data_exporter\ndef register_model(metrics: dict, *args, **kwargs) -> None:\n    \"\"\"\n    Register model in a simple model registry\n    \"\"\"\n    # Create model registry directory\n    registry_path = '/home/src/mlops_demo/model_registry'\n    os.makedirs(registry_path, exist_ok=True)\n    \n    # Create model version\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    version = f\"v_{timestamp}\"\n    \n    # Load the trained model\n    model = joblib.load(metrics['model_path'])\n    scaler = joblib.load('/home/src/mlops_demo/models/scaler.pkl')\n    \n    # Create version directory\n    version_path = os.path.join(registry_path, version)\n    os.makedirs(version_path, exist_ok=True)\n    \n    # Save model artifacts\n    joblib.dump(model, os.path.join(version_path, 'model.pkl'))\n    joblib.dump(scaler, os.path.join(version_path, 'scaler.pkl'))\n    \n    # Update metrics with version info\n    registry_entry = {\n        'version': version,\n        'timestamp': timestamp,\n        'model_type': 'RandomForestClassifier',\n        'status': 'registered',\n        'metrics': {\n            'accuracy': metrics['accuracy'],\n            'auc_score': metrics['auc_score']\n        },\n        'feature_importance': metrics['feature_importance'],\n        'artifacts': {\n            'model_path': os.path.join(version_path, 'model.pkl'),\n            'scaler_path': os.path.join(version_path, 'scaler.pkl')\n        }\n    }\n    \n    # Save registry entry\n    with open(os.path.join(version_path, 'metadata.json'), 'w') as f:\n        json.dump(registry_entry, f, indent=2)\n    \n    # Update latest model pointer\n    latest_path = os.path.join(registry_path, 'latest.json')\n    with open(latest_path, 'w') as f:\n        json.dump({\n            'version': version,\n            'path': version_path,\n            'updated_at': timestamp\n        }, f, indent=2)\n    \n    print(f\"\u2705 Model registered successfully!\")\n    print(f\"Version: {version}\")\n    print(f\"Registry path: {version_path}\")\n    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n    print(f\"AUC Score: {metrics['auc_score']:.4f}\")\n\n@test\ndef test_output(*args) -> None:\n    registry_path = '/home/src/mlops_demo/model_registry'\n    latest_path = os.path.join(registry_path, 'latest.json')\n    assert os.path.exists(latest_path), 'Model registration failed'\n    print(\"\u2705 Model registry validation passed\")\n", "file_path": "/home/src/mlops_demo/data_exporters/model_registry.py", "language": "python", "type": "data_exporter", "uuid": "model_registry"}, "/home/src/mlops_demo/data_exporters/model_deployment.py:data_exporter:python:home/src/mlops demo/data exporters/model deployment": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nimport joblib\nimport json\nimport os\nimport pandas as pd\n\n@data_exporter\ndef deploy_model_for_serving(data: dict, *args, **kwargs) -> None:\n    \"\"\"\n    Simple model deployment - just save deployment info\n    \"\"\"\n    # Create deployment directory\n    deployment_path = '/home/src/mlops_demo/deployment'\n    os.makedirs(deployment_path, exist_ok=True)\n    \n    # Handle both dictionary and list inputs\n    if isinstance(data, list):\n        # If it's a list, assume it's the first item that contains our metrics\n        metrics = data[0] if data else {}\n    else:\n        metrics = data\n    \n    # Extract metrics safely\n    accuracy = metrics.get('accuracy', 0.0)\n    auc_score = metrics.get('auc_score', 0.0)\n    model_path = metrics.get('model_path', '/home/src/mlops_demo/models/churn_model.pkl')\n    \n    # Save deployment metadata\n    deployment_info = {\n        'model_deployed': True,\n        'deployment_timestamp': str(pd.Timestamp.now()),\n        'model_accuracy': float(accuracy),\n        'model_auc': float(auc_score),\n        'model_path': model_path,\n        'status': 'ready_for_predictions'\n    }\n    \n    with open(os.path.join(deployment_path, 'deployment_status.json'), 'w') as f:\n        json.dump(deployment_info, f, indent=2)\n    \n    print(\"\u2705 Model deployed successfully!\")\n    print(f\"   Accuracy: {accuracy:.4f}\")\n    print(f\"   AUC Score: {auc_score:.4f}\")\n    print(f\"   Model Path: {model_path}\")\n    print(f\"   Status: Ready for predictions\")\n    print(f\"   Next: Create prediction pipeline\")\n    \n    # Also print the input data structure for debugging\n    print(f\"\\n\ud83d\udd0d Debug Info:\")\n    print(f\"   Input type: {type(data)}\")\n    if isinstance(data, list):\n        print(f\"   List length: {len(data)}\")\n        if data:\n            print(f\"   First item type: {type(data[0])}\")\n            print(f\"   First item keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not a dict'}\")\n    else:\n        print(f\"   Dict keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}\")\n\n@test\ndef test_output(*args) -> None:\n    deployment_file = '/home/src/mlops_demo/deployment/deployment_status.json'\n    assert os.path.exists(deployment_file), 'Deployment file not created'\n    print(\"\u2705 Model deployment successful\")\n", "file_path": "/home/src/mlops_demo/data_exporters/model_deployment.py", "language": "python", "type": "data_exporter", "uuid": "model_deployment"}, "/home/src/mlops_demo/data_loaders/make_prediction.py:data_loader:python:home/src/mlops demo/data loaders/make prediction": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nimport joblib\nimport json\nimport os\nimport pandas as pd\nimport numpy as np\n\n@data_loader\ndef predict_customer_churn(*args, **kwargs):\n    \"\"\"\n    Make churn predictions using trained model\n    \"\"\"\n    # Get input from API variables or use default\n    input_data = kwargs.get('customer_data', {\n        \"account_age\": 24,\n        \"monthly_charges\": 85.5,\n        \"total_charges\": 2052.0,\n        \"num_services\": 5,\n        \"customer_service_calls\": 2,\n        \"contract_length\": 12,\n        \"payment_method_score\": 0.8,\n        \"usage_frequency\": 0.7,\n        \"support_tickets\": 1,\n        \"satisfaction_score\": 0.6\n    })\n    \n    print(\"\ud83d\udd2e Making Churn Prediction...\")\n    print(f\"Input data: {input_data}\")\n    \n    try:\n        # Load latest model\n        registry_path = '/home/src/mlops_demo/model_registry'\n        latest_file = os.path.join(registry_path, 'latest.json')\n        \n        with open(latest_file, 'r') as f:\n            latest_info = json.load(f)\n        \n        model_path = os.path.join(latest_info['path'], 'model.pkl')\n        scaler_path = os.path.join(latest_info['path'], 'scaler.pkl')\n        \n        # Load model and scaler\n        model = joblib.load(model_path)\n        scaler = joblib.load(scaler_path)\n        \n        # Prepare features\n        feature_names = [\n            'account_age', 'monthly_charges', 'total_charges', 'num_services',\n            'customer_service_calls', 'contract_length', 'payment_method_score',\n            'usage_frequency', 'support_tickets', 'satisfaction_score'\n        ]\n        \n        # Create dataframe\n        df = pd.DataFrame([input_data])\n        \n        # Fill missing features with 0\n        for feature in feature_names:\n            if feature not in df.columns:\n                df[feature] = 0\n        \n        # Select features in correct order\n        X = df[feature_names].values\n        \n        # Scale features\n        X_scaled = scaler.transform(X)\n        \n        # Make prediction\n        prediction = model.predict(X_scaled)[0]\n        probabilities = model.predict_proba(X_scaled)[0]\n        \n        # Create result\n        result = {\n            'customer_id': input_data.get('customer_id', 'unknown'),\n            'prediction': int(prediction),\n            'prediction_text': 'Will Churn' if prediction == 1 else 'Will Stay',\n            'churn_probability': float(probabilities[1]),\n            'stay_probability': float(probabilities[0]),\n            'confidence': float(max(probabilities)),\n            'risk_level': 'High' if probabilities[1] > 0.7 else 'Medium' if probabilities[1] > 0.3 else 'Low',\n            'model_version': latest_info['version'],\n            'timestamp': str(pd.Timestamp.now())\n        }\n        \n        # Print results clearly\n        print(\"\\n\" + \"=\"*50)\n        print(\"\ud83c\udfaf PREDICTION RESULTS\")\n        print(\"=\"*50)\n        print(f\"Customer: {result['customer_id']}\")\n        print(f\"Prediction: {result['prediction_text']}\")\n        print(f\"Risk Level: {result['risk_level']}\")\n        print(f\"Churn Probability: {result['churn_probability']:.1%}\")\n        print(f\"Confidence: {result['confidence']:.1%}\")\n        print(f\"Model Version: {result['model_version']}\")\n        print(\"=\"*50)\n        \n        return result\n        \n    except Exception as e:\n        error_result = {\n            'error': str(e),\n            'status': 'failed',\n            'input_data': input_data\n        }\n        print(f\"\u274c Prediction failed: {e}\")\n        return error_result\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'No prediction output'\n    \n    if 'error' not in output:\n        assert 'prediction' in output, 'Prediction missing'\n        assert 'risk_level' in output, 'Risk level missing'\n        print(\"\u2705 Prediction successful!\")\n    else:\n        print(f\"\u26a0\ufe0f Prediction error: {output['error']}\")\n", "file_path": "/home/src/mlops_demo/data_loaders/make_prediction.py", "language": "python", "type": "data_loader", "uuid": "make_prediction"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}