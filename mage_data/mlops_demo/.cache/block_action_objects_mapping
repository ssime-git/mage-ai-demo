{"block_file": {"charts/make_dataset_histogram_n2.py:chart:python:make dataset histogram n2": {"content": "import pandas as pd\n\nfrom mage_ai.shared.parsers import convert_matrix_to_dataframe\n\n\nif isinstance(df_1, list) and len(df_1) >= 1:\n    item = df_1[0]\n    if isinstance(item, pd.Series):\n        item = item.to_frame()\n    elif not isinstance(item, pd.DataFrame):\n        item = convert_matrix_to_dataframe(item)\n    df_1 = item\n\ncolumns = df_1.columns\ncol = list(filter(lambda x: df_1[x].dtype == float or df_1[x].dtype == int, columns))[0]\nx = df_1[col]\n", "file_path": "charts/make_dataset_histogram_n2.py", "language": "python", "type": "chart", "uuid": "make_dataset_histogram_n2"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/model_deployment.py:data_exporter:python:model deployment": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nimport joblib\nimport json\nimport os\nimport pandas as pd\n\n@data_exporter\ndef deploy_model_for_serving(data: dict, *args, **kwargs) -> None:\n    \"\"\"\n    Simple model deployment - just save deployment info\n    \"\"\"\n    # Create deployment directory\n    deployment_path = '/home/src/mlops_demo/deployment'\n    os.makedirs(deployment_path, exist_ok=True)\n    \n    # Handle both dictionary and list inputs\n    if isinstance(data, list):\n        # If it's a list, assume it's the first item that contains our metrics\n        metrics = data[0] if data else {}\n    else:\n        metrics = data\n    \n    # Extract metrics safely\n    accuracy = metrics.get('accuracy', 0.0)\n    auc_score = metrics.get('auc_score', 0.0)\n    model_path = metrics.get('model_path', '/home/src/mlops_demo/models/churn_model.pkl')\n    \n    # Save deployment metadata\n    deployment_info = {\n        'model_deployed': True,\n        'deployment_timestamp': str(pd.Timestamp.now()),\n        'model_accuracy': float(accuracy),\n        'model_auc': float(auc_score),\n        'model_path': model_path,\n        'status': 'ready_for_predictions'\n    }\n    \n    with open(os.path.join(deployment_path, 'deployment_status.json'), 'w') as f:\n        json.dump(deployment_info, f, indent=2)\n    \n    print(\"\u2705 Model deployed successfully!\")\n    print(f\"   Accuracy: {accuracy:.4f}\")\n    print(f\"   AUC Score: {auc_score:.4f}\")\n    print(f\"   Model Path: {model_path}\")\n    print(f\"   Status: Ready for predictions\")\n    print(f\"   Next: Create prediction pipeline\")\n    \n    # Also print the input data structure for debugging\n    print(f\"\\n\ud83d\udd0d Debug Info:\")\n    print(f\"   Input type: {type(data)}\")\n    if isinstance(data, list):\n        print(f\"   List length: {len(data)}\")\n        if data:\n            print(f\"   First item type: {type(data[0])}\")\n            print(f\"   First item keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Not a dict'}\")\n    else:\n        print(f\"   Dict keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}\")\n\n@test\ndef test_output(*args) -> None:\n    deployment_file = '/home/src/mlops_demo/deployment/deployment_status.json'\n    assert os.path.exists(deployment_file), 'Deployment file not created'\n    print(\"\u2705 Model deployment successful\")\n", "file_path": "data_exporters/model_deployment.py", "language": "python", "type": "data_exporter", "uuid": "model_deployment"}, "data_exporters/model_registry.py:data_exporter:python:model registry": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\nimport json\nimport os\nfrom datetime import datetime\nimport joblib\n\n@data_exporter\ndef register_model(metrics: dict, *args, **kwargs) -> None:\n    \"\"\"\n    Register model in a simple model registry\n    \"\"\"\n    # Create model registry directory\n    registry_path = '/home/src/mlops_demo/model_registry'\n    os.makedirs(registry_path, exist_ok=True)\n    \n    # Create model version\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    version = f\"v_{timestamp}\"\n    \n    # Load the trained model\n    model = joblib.load(metrics['model_path'])\n    scaler = joblib.load('/home/src/mlops_demo/models/scaler.pkl')\n    \n    # Create version directory\n    version_path = os.path.join(registry_path, version)\n    os.makedirs(version_path, exist_ok=True)\n    \n    # Save model artifacts\n    joblib.dump(model, os.path.join(version_path, 'model.pkl'))\n    joblib.dump(scaler, os.path.join(version_path, 'scaler.pkl'))\n    \n    # Update metrics with version info\n    registry_entry = {\n        'version': version,\n        'timestamp': timestamp,\n        'model_type': 'RandomForestClassifier',\n        'status': 'registered',\n        'metrics': {\n            'accuracy': metrics['accuracy'],\n            'auc_score': metrics['auc_score']\n        },\n        'feature_importance': metrics['feature_importance'],\n        'artifacts': {\n            'model_path': os.path.join(version_path, 'model.pkl'),\n            'scaler_path': os.path.join(version_path, 'scaler.pkl')\n        }\n    }\n    \n    # Save registry entry\n    with open(os.path.join(version_path, 'metadata.json'), 'w') as f:\n        json.dump(registry_entry, f, indent=2)\n    \n    # Update latest model pointer\n    latest_path = os.path.join(registry_path, 'latest.json')\n    with open(latest_path, 'w') as f:\n        json.dump({\n            'version': version,\n            'path': version_path,\n            'updated_at': timestamp\n        }, f, indent=2)\n    \n    print(f\"\u2705 Model registered successfully!\")\n    print(f\"Version: {version}\")\n    print(f\"Registry path: {version_path}\")\n    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n    print(f\"AUC Score: {metrics['auc_score']:.4f}\")\n\n@test\ndef test_output(*args) -> None:\n    registry_path = '/home/src/mlops_demo/model_registry'\n    latest_path = os.path.join(registry_path, 'latest.json')\n    assert os.path.exists(latest_path), 'Model registration failed'\n    print(\"\u2705 Model registry validation passed\")\n", "file_path": "data_exporters/model_registry.py", "language": "python", "type": "data_exporter", "uuid": "model_registry"}, "data_exporters/online_prediction.py:data_exporter:python:online prediction": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\nimport joblib\nimport json\nimport os\nfrom datetime import datetime\n\n@data_exporter\ndef create_prediction_utility(metrics: dict, *args, **kwargs) -> None:\n    \"\"\"\n    Create a simple prediction utility function\n    \"\"\"\n    # Create prediction utility code\n    prediction_code = f'''import joblib\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\ndef predict_churn(input_data):\n    \"\"\"\n    Simple prediction function for churn prediction\n    \"\"\"\n    try:\n        # Load latest model from registry\n        registry_path = '/home/src/mlops_demo/model_registry'\n        latest_path = os.path.join(registry_path, 'latest.json')\n        \n        if not os.path.exists(latest_path):\n            return {{\n                'error': 'No trained model found. Please run the training pipeline first.',\n                'status': 'error'\n            }}\n        \n        with open(latest_path, 'r') as f:\n            latest_info = json.load(f)\n        \n        # Load model and scaler\n        model_path = os.path.join(latest_info['path'], 'model.pkl')\n        scaler_path = os.path.join(latest_info['path'], 'scaler.pkl')\n        \n        model = joblib.load(model_path)\n        scaler = joblib.load(scaler_path)\n        \n        # Feature names\n        feature_names = [\n            'account_age', 'monthly_charges', 'total_charges', 'num_services',\n            'customer_service_calls', 'contract_length', 'payment_method_score',\n            'usage_frequency', 'support_tickets', 'satisfaction_score'\n        ]\n        \n        # Prepare input data\n        df = pd.DataFrame([input_data])\n        \n        # Ensure all features are present\n        for feature in feature_names:\n            if feature not in df.columns:\n                df[feature] = 0\n        \n        # Select and order features\n        X = df[feature_names].values\n        \n        # Scale features\n        X_scaled = scaler.transform(X)\n        \n        # Make prediction\n        prediction = model.predict(X_scaled)[0]\n        probability = model.predict_proba(X_scaled)[0]\n        \n        result = {{\n            'input_data': input_data,\n            'prediction': int(prediction),\n            'prediction_label': 'Will Churn' if prediction == 1 else 'Will Not Churn',\n            'probability': {{\n                'no_churn': float(probability[0]),\n                'churn': float(probability[1])\n            }},\n            'confidence': float(max(probability)),\n            'risk_level': 'High' if probability[1] > 0.7 else 'Medium' if probability[1] > 0.3 else 'Low',\n            'model_version': latest_info['version'],\n            'prediction_timestamp': datetime.now().isoformat(),\n            'status': 'success'\n        }}\n        \n        return result\n        \n    except Exception as e:\n        return {{\n            'error': str(e),\n            'status': 'error'\n        }}\n\ndef batch_predict(data_list):\n    \"\"\"\n    Make predictions for multiple customers\n    \"\"\"\n    results = []\n    for i, customer_data in enumerate(data_list):\n        print(f\"Processing customer {{i+1}}/{{len(data_list)}}...\")\n        result = predict_churn(customer_data)\n        results.append(result)\n    return results\n\nif __name__ == \"__main__\":\n    # Test the function\n    test_data = {{\n        \"account_age\": 36,\n        \"monthly_charges\": 120.0,\n        \"total_charges\": 4320.0,\n        \"num_services\": 8,\n        \"customer_service_calls\": 5,\n        \"contract_length\": 6,\n        \"payment_method_score\": 0.3,\n        \"usage_frequency\": 0.4,\n        \"support_tickets\": 3,\n        \"satisfaction_score\": 0.2\n    }}\n    \n    print(\"Testing Prediction Function...\")\n    print(\"=\" * 50)\n    result = predict_churn(test_data)\n    print(json.dumps(result, indent=2))\n    print(\"=\" * 50)\n'''\n    \n    # Save the prediction utility\n    utility_path = '/home/src/mlops_demo/predict.py'\n    with open(utility_path, 'w') as f:\n        f.write(prediction_code)\n    \n    # Create a simple test script\n    test_script = f'''#!/usr/bin/env python3\nimport sys\nimport os\nsys.path.append('/home/src/mlops_demo')\n\nfrom predict import predict_churn\nimport json\n\n# Test cases\ntest_cases = [\n    {{\n        \"name\": \"High Risk Customer\",\n        \"data\": {{\n            \"account_age\": 36,\n            \"monthly_charges\": 120.0,\n            \"total_charges\": 4320.0,\n            \"num_services\": 8,\n            \"customer_service_calls\": 5,\n            \"contract_length\": 6,\n            \"payment_method_score\": 0.3,\n            \"usage_frequency\": 0.4,\n            \"support_tickets\": 3,\n            \"satisfaction_score\": 0.2\n        }}\n    }},\n    {{\n        \"name\": \"Low Risk Customer\",\n        \"data\": {{\n            \"account_age\": 48,\n            \"monthly_charges\": 75.0,\n            \"total_charges\": 3600.0,\n            \"num_services\": 3,\n            \"customer_service_calls\": 1,\n            \"contract_length\": 24,\n            \"payment_method_score\": 0.9,\n            \"usage_frequency\": 0.8,\n            \"support_tickets\": 0,\n            \"satisfaction_score\": 0.9\n        }}\n    }}\n]\n\nprint(\"\ud83d\udd2e Mage AI Churn Prediction Service\")\nprint(\"=\" * 50)\n\nfor test_case in test_cases:\n    print(f\"\\\\n\ud83d\udcca Testing: {{test_case['name']}}\")\n    print(\"-\" * 30)\n    result = predict_churn(test_case['data'])\n    \n    if result['status'] == 'success':\n        print(f\"Prediction: {{result['prediction_label']}}\")\n        print(f\"Risk Level: {{result['risk_level']}}\")\n        print(f\"Confidence: {{result['confidence']:.2%}}\")\n        print(f\"Churn Probability: {{result['probability']['churn']:.2%}}\")\n    else:\n        print(f\"Error: {{result['error']}}\")\n\nprint(\"\\\\n\" + \"=\" * 50)\nprint(\"\u2705 Prediction service is ready!\")\nprint(\"\ud83d\udca1 Usage: from predict import predict_churn\")\n'''\n    \n    test_script_path = '/home/src/mlops_demo/test_predictions.py'\n    with open(test_script_path, 'w') as f:\n        f.write(test_script)\n    \n    # Make it executable\n    os.chmod(test_script_path, 0o755)\n    \n    print(f\"\u2705 Prediction utility created successfully!\")\n    print(f\"\ud83d\udcc1 Utility file: {utility_path}\")\n    print(f\"\ud83e\uddea Test script: {test_script_path}\")\n    print(f\"\")\n    print(f\"\ud83d\ude80 To test your prediction service:\")\n    print(f\"   cd /home/src/mlops_demo\")\n    print(f\"   python test_predictions.py\")\n    print(f\"\")\n    print(f\"\ud83d\udcdd To use in code:\")\n    print(f\"   from predict import predict_churn\")\n    print(f\"   result = predict_churn(customer_data)\")\n    \n    # Test the function immediately\n    print(f\"\")\n    print(f\"\ud83d\udd0d Quick test...\")\n    try:\n        exec(open(utility_path).read())\n        test_data = {\n            \"account_age\": 36,\n            \"monthly_charges\": 120.0,\n            \"total_charges\": 4320.0,\n            \"num_services\": 8,\n            \"customer_service_calls\": 5,\n            \"contract_length\": 6,\n            \"payment_method_score\": 0.3,\n            \"usage_frequency\": 0.4,\n            \"support_tickets\": 3,\n            \"satisfaction_score\": 0.2\n        }\n        # Note: predict_churn function is now available in local scope\n        result = predict_churn(test_data)\n        if result['status'] == 'success':\n            print(f\"\u2705 Test prediction successful!\")\n            print(f\"   Prediction: {result['prediction_label']}\")\n            print(f\"   Risk Level: {result['risk_level']}\")\n        else:\n            print(f\"\u274c Test failed: {result['error']}\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Quick test failed: {str(e)}\")\n        print(f\"   Run the test script manually after pipeline completion\")\n\n@test\ndef test_output(*args) -> None:\n    utility_path = '/home/src/mlops_demo/predict.py'\n    test_path = '/home/src/mlops_demo/test_predictions.py'\n    assert os.path.exists(utility_path), 'Prediction utility not created'\n    assert os.path.exists(test_path), 'Test script not created'\n    print(\"\u2705 Prediction utility validation passed\")\n", "file_path": "data_exporters/online_prediction.py", "language": "python", "type": "data_exporter", "uuid": "online_prediction"}, "data_exporters/prediction_svc.py:data_exporter:python:prediction svc": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\nimport joblib\nimport json\nimport os\nfrom flask import Flask, request, jsonify\nimport pandas as pd\nimport numpy as np\n\n@data_exporter\ndef create_prediction_service(metrics: dict, *args, **kwargs) -> None:\n    \"\"\"\n    Create a simple prediction service\n    \"\"\"\n    # Create service directory\n    service_path = '/home/src/mlops_demo/prediction_service'\n    os.makedirs(service_path, exist_ok=True)\n    \n    # Create Flask app code\n    flask_app_code = '''\nimport joblib\nimport json\nimport os\nfrom flask import Flask, request, jsonify\nimport pandas as pd\nimport numpy as np\n\napp = Flask(__name__)\n\n# Load latest model\nwith open('/home/src/mlops_demo/model_registry/latest.json', 'r') as f:\n    latest_info = json.load(f)\n\nmodel_path = os.path.join(latest_info['path'], 'model.pkl')\nscaler_path = os.path.join(latest_info['path'], 'scaler.pkl')\n\nmodel = joblib.load(model_path)\nscaler = joblib.load(scaler_path)\n\nfeature_names = [\n    'account_age', 'monthly_charges', 'total_charges', 'num_services',\n    'customer_service_calls', 'contract_length', 'payment_method_score',\n    'usage_frequency', 'support_tickets', 'satisfaction_score'\n]\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({'status': 'healthy', 'version': latest_info['version']})\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    try:\n        data = request.json\n        \n        # Convert to DataFrame\n        df = pd.DataFrame([data])\n        \n        # Ensure all features are present\n        for feature in feature_names:\n            if feature not in df.columns:\n                df[feature] = 0\n        \n        # Select and order features\n        X = df[feature_names]\n        \n        # Scale features\n        X_scaled = scaler.transform(X)\n        \n        # Make prediction\n        prediction = model.predict(X_scaled)[0]\n        probability = model.predict_proba(X_scaled)[0]\n        \n        return jsonify({\n            'prediction': int(prediction),\n            'probability': {\n                'no_churn': float(probability[0]),\n                'churn': float(probability[1])\n            },\n            'risk_level': 'High' if probability[1] > 0.7 else 'Medium' if probability[1] > 0.3 else 'Low'\n        })\n    \n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n'''\n    \n    # Save Flask app\n    with open(os.path.join(service_path, 'app.py'), 'w') as f:\n        f.write(flask_app_code)\n    \n    # Create requirements.txt\n    requirements = '''flask==2.3.3\njoblib==1.3.2\npandas==2.0.3\nnumpy==1.24.3\nscikit-learn==1.3.0\n'''\n    \n    with open(os.path.join(service_path, 'requirements.txt'), 'w') as f:\n        f.write(requirements)\n    \n    # Create test script\n    test_script = '''\nimport requests\nimport json\n\n# Test data\ntest_customer = {\n    \"account_age\": 24,\n    \"monthly_charges\": 85.5,\n    \"total_charges\": 2052.0,\n    \"num_services\": 5,\n    \"customer_service_calls\": 2,\n    \"contract_length\": 12,\n    \"payment_method_score\": 0.8,\n    \"usage_frequency\": 0.7,\n    \"support_tickets\": 1,\n    \"satisfaction_score\": 0.6\n}\n\n# Make prediction request\nresponse = requests.post('http://localhost:5000/predict', \n                        json=test_customer,\n                        headers={'Content-Type': 'application/json'})\n\nprint(\"Prediction Result:\")\nprint(json.dumps(response.json(), indent=2))\n'''\n    \n    with open(os.path.join(service_path, 'test_prediction.py'), 'w') as f:\n        f.write(test_script)\n    \n    print(f\"\u2705 Prediction service created!\")\n    print(f\"Service path: {service_path}\")\n    print(f\"To start the service:\")\n    print(f\"  cd {service_path}\")\n    print(f\"  pip install -r requirements.txt\")\n    print(f\"  python app.py\")\n\n@test\ndef test_output(*args) -> None:\n    service_path = '/home/src/mlops_demo/prediction_service'\n    app_path = os.path.join(service_path, 'app.py')\n    assert os.path.exists(app_path), 'Prediction service creation failed'\n    print(\"\u2705 Prediction service validation passed\")\n", "file_path": "data_exporters/prediction_svc.py", "language": "python", "type": "data_exporter", "uuid": "prediction_svc"}, "data_loaders/load_model_and_make_prediction.py:data_loader:python:load model and make prediction": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nimport joblib\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\n\n@data_loader\ndef predict_churn(*args, **kwargs):\n    \"\"\"\n    Load model and make predictions via API trigger\n    \"\"\"\n    # Get prediction data from API call variables\n    if 'input_data' in kwargs:\n        input_data = kwargs['input_data']\n    else:\n        # Default test data for demonstration\n        input_data = {\n            \"account_age\": 24,\n            \"monthly_charges\": 85.5,\n            \"total_charges\": 2052.0,\n            \"num_services\": 5,\n            \"customer_service_calls\": 2,\n            \"contract_length\": 12,\n            \"payment_method_score\": 0.8,\n            \"usage_frequency\": 0.7,\n            \"support_tickets\": 1,\n            \"satisfaction_score\": 0.6\n        }\n    \n    # Load latest model from registry\n    registry_path = '/home/src/mlops_demo/model_registry'\n    latest_path = os.path.join(registry_path, 'latest.json')\n    \n    if not os.path.exists(latest_path):\n        raise Exception(\"No trained model found. Please run the training pipeline first.\")\n    \n    with open(latest_path, 'r') as f:\n        latest_info = json.load(f)\n    \n    # Load model and scaler\n    model_path = os.path.join(latest_info['path'], 'model.pkl')\n    scaler_path = os.path.join(latest_info['path'], 'scaler.pkl')\n    \n    model = joblib.load(model_path)\n    scaler = joblib.load(scaler_path)\n    \n    # Feature names (should match training)\n    feature_names = [\n        'account_age', 'monthly_charges', 'total_charges', 'num_services',\n        'customer_service_calls', 'contract_length', 'payment_method_score',\n        'usage_frequency', 'support_tickets', 'satisfaction_score'\n    ]\n    \n    # Prepare input data\n    df = pd.DataFrame([input_data])\n    \n    # Ensure all features are present\n    for feature in feature_names:\n        if feature not in df.columns:\n            df[feature] = 0\n    \n    # Select and order features\n    X = df[feature_names].values\n    \n    # Scale features\n    X_scaled = scaler.transform(X)\n    \n    # Make prediction\n    prediction = model.predict(X_scaled)[0]\n    probability = model.predict_proba(X_scaled)[0]\n    \n    result = {\n        'customer_data': input_data,\n        'prediction': int(prediction),\n        'probability': {\n            'no_churn': float(probability[0]),\n            'churn': float(probability[1])\n        },\n        'risk_level': 'High' if probability[1] > 0.7 else 'Medium' if probability[1] > 0.3 else 'Low',\n        'model_version': latest_info['version'],\n        'prediction_timestamp': pd.Timestamp.now().isoformat()\n    }\n    \n    print(f\"Prediction made: {result}\")\n    return result\n\n@test\ndef test_output(output, *args) -> None:\n    assert 'prediction' in output, 'Prediction missing'\n    assert 'probability' in output, 'Probability missing'\n    assert 'risk_level' in output, 'Risk level missing'\n    print(\"\u2705 Prediction service validation passed\")\n", "file_path": "data_loaders/load_model_and_make_prediction.py", "language": "python", "type": "data_loader", "uuid": "load_model_and_make_prediction"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/make_dataset.py:data_loader:python:make dataset": {"content": "import io\nimport pandas as pd\nimport requests\nimport numpy as np\nfrom sklearn.datasets import make_classification\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_customer_data(*args, **kwargs):\n    \"\"\"\n    Generate synthetic customer data for churn prediction\n    \"\"\"\n    # Generate synthetic dataset\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=10,\n        n_informative=8,\n        n_redundant=2,\n        n_clusters_per_class=1,\n        random_state=42\n    )\n    \n    # Create DataFrame with meaningful column names\n    feature_names = [\n        'account_age', 'monthly_charges', 'total_charges', 'num_services',\n        'customer_service_calls', 'contract_length', 'payment_method_score',\n        'usage_frequency', 'support_tickets', 'satisfaction_score'\n    ]\n    \n    df = pd.DataFrame(X, columns=feature_names)\n    df['customer_id'] = range(1, len(df) + 1)\n    df['churn'] = y\n    \n    # Add some realistic data transformations\n    df['account_age'] = np.abs(df['account_age'] * 12).astype(int)  # months\n    df['monthly_charges'] = np.abs(df['monthly_charges'] * 50 + 100)  # dollars\n    df['total_charges'] = df['monthly_charges'] * df['account_age']\n    df['num_services'] = np.abs(df['num_services']).astype(int) % 10 + 1\n    \n    print(f\"Loaded {len(df)} customer records\")\n    print(f\"Churn rate: {df['churn'].mean():.2%}\")\n    \n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'Data loading failed'\n    assert len(output) > 0, 'No data loaded'\n    assert 'churn' in output.columns, 'Target variable missing'\n    print(f\"\u2705 Data validation passed: {len(output)} records loaded\")\n", "file_path": "data_loaders/make_dataset.py", "language": "python", "type": "data_loader", "uuid": "make_dataset"}, "data_loaders/make_prediction.py:data_loader:python:make prediction": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nimport joblib\nimport json\nimport os\nimport pandas as pd\nimport numpy as np\n\n@data_loader\ndef predict_customer_churn(*args, **kwargs):\n    \"\"\"\n    Make churn predictions using trained model\n    \"\"\"\n    # Get input from API variables or use default\n    input_data = kwargs.get('customer_data', {\n        \"account_age\": 24,\n        \"monthly_charges\": 85.5,\n        \"total_charges\": 2052.0,\n        \"num_services\": 5,\n        \"customer_service_calls\": 2,\n        \"contract_length\": 12,\n        \"payment_method_score\": 0.8,\n        \"usage_frequency\": 0.7,\n        \"support_tickets\": 1,\n        \"satisfaction_score\": 0.6\n    })\n    \n    print(\"\ud83d\udd2e Making Churn Prediction...\")\n    print(f\"Input data: {input_data}\")\n    \n    try:\n        # Load latest model\n        registry_path = '/home/src/mlops_demo/model_registry'\n        latest_file = os.path.join(registry_path, 'latest.json')\n        \n        with open(latest_file, 'r') as f:\n            latest_info = json.load(f)\n        \n        model_path = os.path.join(latest_info['path'], 'model.pkl')\n        scaler_path = os.path.join(latest_info['path'], 'scaler.pkl')\n        \n        # Load model and scaler\n        model = joblib.load(model_path)\n        scaler = joblib.load(scaler_path)\n        \n        # Prepare features\n        feature_names = [\n            'account_age', 'monthly_charges', 'total_charges', 'num_services',\n            'customer_service_calls', 'contract_length', 'payment_method_score',\n            'usage_frequency', 'support_tickets', 'satisfaction_score'\n        ]\n        \n        # Create dataframe\n        df = pd.DataFrame([input_data])\n        \n        # Fill missing features with 0\n        for feature in feature_names:\n            if feature not in df.columns:\n                df[feature] = 0\n        \n        # Select features in correct order\n        X = df[feature_names].values\n        \n        # Scale features\n        X_scaled = scaler.transform(X)\n        \n        # Make prediction\n        prediction = model.predict(X_scaled)[0]\n        probabilities = model.predict_proba(X_scaled)[0]\n        \n        # Create result\n        result = {\n            'customer_id': input_data.get('customer_id', 'unknown'),\n            'prediction': int(prediction),\n            'prediction_text': 'Will Churn' if prediction == 1 else 'Will Stay',\n            'churn_probability': float(probabilities[1]),\n            'stay_probability': float(probabilities[0]),\n            'confidence': float(max(probabilities)),\n            'risk_level': 'High' if probabilities[1] > 0.7 else 'Medium' if probabilities[1] > 0.3 else 'Low',\n            'model_version': latest_info['version'],\n            'timestamp': str(pd.Timestamp.now())\n        }\n        \n        # Print results clearly\n        print(\"\\n\" + \"=\"*50)\n        print(\"\ud83c\udfaf PREDICTION RESULTS\")\n        print(\"=\"*50)\n        print(f\"Customer: {result['customer_id']}\")\n        print(f\"Prediction: {result['prediction_text']}\")\n        print(f\"Risk Level: {result['risk_level']}\")\n        print(f\"Churn Probability: {result['churn_probability']:.1%}\")\n        print(f\"Confidence: {result['confidence']:.1%}\")\n        print(f\"Model Version: {result['model_version']}\")\n        print(\"=\"*50)\n        \n        return result\n        \n    except Exception as e:\n        error_result = {\n            'error': str(e),\n            'status': 'failed',\n            'input_data': input_data\n        }\n        print(f\"\u274c Prediction failed: {e}\")\n        return error_result\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'No prediction output'\n    \n    if 'error' not in output:\n        assert 'prediction' in output, 'Prediction missing'\n        assert 'risk_level' in output, 'Risk level missing'\n        print(\"\u2705 Prediction successful!\")\n    else:\n        print(f\"\u26a0\ufe0f Prediction error: {output['error']}\")\n", "file_path": "data_loaders/make_prediction.py", "language": "python", "type": "data_loader", "uuid": "make_prediction"}, "data_loaders/spiritual_night.py:data_loader:python:spiritual night": {"content": "from mage_ai.orchestration.triggers.api import trigger_pipeline\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef trigger(*args, **kwargs):\n    \"\"\"\n    Trigger another pipeline to run.\n\n    Documentation: https://docs.mage.ai/orchestration/triggers/trigger-pipeline\n    \"\"\"\n\n    trigger_pipeline(\n        'pipeline_uuid',        # Required: enter the UUID of the pipeline to trigger\n        variables={},           # Optional: runtime variables for the pipeline\n        check_status=False,     # Optional: poll and check the status of the triggered pipeline\n        error_on_failure=False, # Optional: if triggered pipeline fails, raise an exception\n        poll_interval=60,       # Optional: check the status of triggered pipeline every N seconds\n        poll_timeout=None,      # Optional: raise an exception after N seconds\n        verbose=True,           # Optional: print status of triggered pipeline run\n    )\n", "file_path": "data_loaders/spiritual_night.py", "language": "python", "type": "data_loader", "uuid": "spiritual_night"}, "markdowns/how_to_use.md:markdown:markdown:how to use": {"content": "# How to use\n\n```bash\n# Test prediction API\ncurl -X POST http://localhost:6789/api/pipeline_schedules/1/pipeline_runs/c92cc53194484103a63d3cf4446a3b4f \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"pipeline_run\": {\n      \"variables\": {\n        \"input_data\": {\n          \"account_age\": 36,\n          \"monthly_charges\": 120.0,\n          \"total_charges\": 4320.0,\n          \"num_services\": 8,\n          \"customer_service_calls\": 5,\n          \"contract_length\": 6,\n          \"payment_method_score\": 0.3,\n          \"usage_frequency\": 0.4,\n          \"support_tickets\": 3,\n          \"satisfaction_score\": 0.2\n        }\n      }\n    }\n  }'\n```", "file_path": "markdowns/how_to_use.md", "language": "markdown", "type": "markdown", "uuid": "how_to_use"}, "transformers/data_preproecessing.py:transformer:python:data preproecessing": {"content": "import joblib\nimport os\n\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef preprocess_data(df: DataFrame, *args, **kwargs) -> dict:\n    \"\"\"\n    Preprocess customer data for ML training\n    \"\"\"\n    # Create preprocessing directory\n    os.makedirs('/home/src/mlops_demo/models', exist_ok=True)\n    \n    # Separate features and target\n    feature_cols = [col for col in df.columns if col not in ['customer_id', 'churn']]\n    X = df[feature_cols]\n    y = df['churn']\n    \n    # Handle missing values\n    X = X.fillna(X.median())\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Save the scaler for later use\n    joblib.dump(scaler, '/home/src/mlops_demo/models/scaler.pkl')\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    print(f\"Training set: {X_train.shape[0]} samples\")\n    print(f\"Test set: {X_test.shape[0]} samples\")\n    print(f\"Features: {len(feature_cols)}\")\n    \n    # Convert numpy arrays to lists for JSON serialization\n    # But keep the original arrays for ML training\n    return {\n        'X_train': X_train.tolist(),\n        'X_test': X_test.tolist(),\n        'y_train': y_train.tolist(),\n        'y_test': y_test.tolist(),\n        'feature_names': feature_cols,\n        'scaler_path': '/home/src/mlops_demo/models/scaler.pkl',\n        'data_shapes': {\n            'X_train_shape': X_train.shape,\n            'X_test_shape': X_test.shape,\n            'y_train_shape': y_train.shape,\n            'y_test_shape': y_test.shape\n        }\n    }\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'Output is None'\n    assert isinstance(output, dict), 'Output should be a dictionary'\n    assert 'X_train' in output, 'Training features missing'\n    assert 'y_train' in output, 'Training target missing'\n    assert 'X_test' in output, 'Test features missing'\n    assert 'y_test' in output, 'Test target missing'\n    assert 'data_shapes' in output, 'Data shapes missing'\n    \n    # Check data shapes using the stored shape info\n    shapes = output['data_shapes']\n    assert shapes['X_train_shape'][0] > 0, 'No training data'\n    assert shapes['X_test_shape'][0] > 0, 'No test data'\n    assert shapes['y_train_shape'][0] > 0, 'No training labels'\n    assert shapes['y_test_shape'][0] > 0, 'No test labels'\n    \n    # Check that data is in list format (JSON serializable)\n    assert isinstance(output['X_train'], list), 'X_train should be a list'\n    assert isinstance(output['y_train'], list), 'y_train should be a list'\n    \n    print(f\"\u2705 Preprocessing validation passed\")\n    print(f\"   Training samples: {shapes['X_train_shape'][0]}\")\n    print(f\"   Test samples: {shapes['X_test_shape'][0]}\")\n    print(f\"   Features: {len(output['feature_names'])}\")\n\n", "file_path": "transformers/data_preproecessing.py", "language": "python", "type": "transformer", "uuid": "data_preproecessing"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/model_training.py:transformer:python:model training": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\nimport joblib\nimport json\nimport os\n\n@transformer\ndef train_model(data: dict, *args, **kwargs) -> dict:\n    \"\"\"\n    Train machine learning model for churn prediction\n    \"\"\"\n    # Convert lists back to numpy arrays for ML training\n    import numpy as np\n    X_train = np.array(data['X_train'])\n    y_train = np.array(data['y_train'])\n    X_test = np.array(data['X_test'])\n    y_test = np.array(data['y_test'])\n    \n    # Initialize and train model\n    model = RandomForestClassifier(\n        n_estimators=100,\n        max_depth=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    print(\"Training model...\")\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_pred_proba)\n    \n    # Feature importance\n    feature_importance = dict(zip(\n        data['feature_names'], \n        model.feature_importances_\n    ))\n    \n    # Sort by importance\n    feature_importance = dict(sorted(\n        feature_importance.items(), \n        key=lambda x: x[1], \n        reverse=True\n    ))\n    \n    # Save model\n    model_path = '/home/src/mlops_demo/models/churn_model.pkl'\n    joblib.dump(model, model_path)\n    \n    # Save metrics\n    metrics = {\n        'accuracy': float(accuracy),\n        'auc_score': float(auc_score),\n        'feature_importance': feature_importance,\n        'model_path': model_path,\n        'training_samples': len(X_train),\n        'test_samples': len(X_test)\n    }\n    \n    with open('/home/src/mlops_demo/models/metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    print(f\"Model trained successfully!\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"AUC Score: {auc_score:.4f}\")\n    print(f\"Top 3 features: {list(feature_importance.keys())[:3]}\")\n    \n    return metrics\n\n@test\ndef test_output(output, *args) -> None:\n    assert 'accuracy' in output, 'Accuracy metric missing'\n    assert output['accuracy'] > 0.5, 'Model accuracy too low'\n    assert 'model_path' in output, 'Model path missing'\n    print(f\"\u2705 Model training validation passed\")\n", "file_path": "transformers/model_training.py", "language": "python", "type": "transformer", "uuid": "model_training"}, "pipelines/asynch_prediction_pipeline/metadata.yaml:pipeline:yaml:asynch prediction pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Load Model and Make Prediction\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_model_and_make_prediction\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: markdown\n  name: How to use\n  retry_config: null\n  status: updated\n  timeout: null\n  type: markdown\n  upstream_blocks: []\n  uuid: how_to_use\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-22 07:28:23.958212+00:00'\ndata_integration: null\ndescription: Prediction SVC\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: asynch prediction pipeline\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: asynch_prediction_pipeline\nvariables_dir: /home/src/mage_data/mlops_demo\nwidgets: []\n", "file_path": "pipelines/asynch_prediction_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "asynch_prediction_pipeline/metadata"}, "pipelines/asynch_prediction_pipeline/__init__.py:pipeline:python:asynch prediction pipeline/  init  ": {"content": "", "file_path": "pipelines/asynch_prediction_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "asynch_prediction_pipeline/__init__"}, "pipelines/demo_mlops/metadata.yaml:pipeline:yaml:demo mlops/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - make_dataset_histogram_n2\n  - data_preproecessing\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: make dataset\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: make_dataset\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - model_training\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Data Preproecessing\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - make_dataset\n  uuid: data_preproecessing\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - model_registry\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Model training\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - data_preproecessing\n  uuid: model_training\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/model_registry.py\n    file_source:\n      path: data_exporters/model_registry.py\n  downstream_blocks:\n  - model_deployment\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Model registry\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - model_training\n  uuid: model_registry\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: model_deployment\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - model_registry\n  uuid: model_deployment\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-22 06:52:01.545651+00:00'\ndata_integration: null\ndescription: Simple MLOPS Pipeline for demo\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: demo_mlops\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: demo_mlops\nvariables_dir: /home/src/mage_data/mlops_demo\nwidgets:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    buckets: 10\n    chart_type: histogram\n    group_by:\n    - account_age\n    width_percentage: '1'\n    x: x\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: make_dataset_histogram_n2\n  retry_config: null\n  status: executed\n  timeout: null\n  type: chart\n  upstream_blocks:\n  - make_dataset\n  uuid: make_dataset_histogram_n2\n", "file_path": "pipelines/demo_mlops/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "demo_mlops/metadata"}, "pipelines/demo_mlops/__init__.py:pipeline:python:demo mlops/  init  ": {"content": "", "file_path": "pipelines/demo_mlops/__init__.py", "language": "python", "type": "pipeline", "uuid": "demo_mlops/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/online_prediction/metadata.yaml:pipeline:yaml:online prediction/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: make_prediction\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: make_prediction\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-05-22 08:05:34.286161+00:00'\ndata_integration: null\ndescription: Online prediction\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: online prediction\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: online_prediction\nvariables_dir: /home/src/mage_data/mlops_demo\nwidgets: []\n", "file_path": "pipelines/online_prediction/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "online_prediction/metadata"}, "pipelines/online_prediction/__init__.py:pipeline:python:online prediction/  init  ": {"content": "", "file_path": "pipelines/online_prediction/__init__.py", "language": "python", "type": "pipeline", "uuid": "online_prediction/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}